The goal is to make sure the patterns we see are genuine and not just random events. In this section, we explain how we used statistics to confirm the reliability of each pattern.

**Analyzing Success Rates**

To get a clear picture of how well certain patterns like the double bottom, double top, triple bottom, triple top, and head and shoulders work, we checked their success rates. Here's the formula we used:
$$ \text{Success Rate} = \frac{\text{Successful occurrences of pattern}}{\text{Total occurrences of pattern}} $$

We used both the main data and a set of random data for this, so we could see how the patterns stacked up against random choices.

**Proportions Z-Test: Checking Predictive Power**

We used the proportions z-test to see how the success rates of the patterns compare to random events. If we get a high z-score, it means the patterns are doing better than random choices. We also used p-values to show how sure we can be about these findings, based on the idea that there's no real difference between the patterns and random events.

**Bonferroni Correction: Making Adjustments for Many Tests**

Because we looked at so many patterns and settings, we had to make some adjustments to the p-values. This helps us avoid mistakenly seeing a pattern as important just because we did so many tests. For this, we used the Bonferroni correction, which is a trusted way to make these adjustments when you're doing a lot of comparisons.

**Effect Size: Understanding the Real Impact**

It is one thing to know there's a difference, but we also need to know how big that difference is. To do this, we calculated the effect size using Cohenâ€™s $h$. This helps us see not just if there's a meaningful difference between the patterns and random events, but also how big that difference really is. By doing this, we get a complete understanding of what the results mean.
