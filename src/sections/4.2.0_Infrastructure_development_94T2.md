Rather than relying on existing scripts or available pipelining tools such as Apache Spark [@spark], Apache Kafka [@kafka], and traditional ETL (Extract, Transform, Load) solutions, the choice has been made to develop a specialized pipeline for processing stock data. These aforementioned tools, while powerful and versatile, may introduce complexities and overheads not necessarily optimized for the specific use-case. This decision is driven by the requirement to swiftly obtain the essential data for the subsequent analysis stages of the research. The custom pipeline has been meticulously designed to optimize performance through the effective utilization of caching and processing methods, which will be elaborated upon in the forthcoming section.