<!-- Intro -->

In the previous chapter, we discussed how the pipeline is set up. Now that we have a near finished pipeline, it is time to make use of it by writing scripts that can be evaluated. Since these scripts are tightly integrated, these can be evaluated much faster than if they run as part of a large monolith program.

These scripts are critical to the methodology, as they are the gateway to quickly processing market in minutes which would otherwise take hours. The benefit of the fast processing time is that we can quickly iterate the results, and test on a wider range of hyperparameters. We will have a brief overview on how these scripts work, and explain them in detail in their respective methodology chapter.

Since we need features to do any sort of pattern extracting, these scripts will provide the means to do so. Scripts produce temporal events, and these events can be used to determine if a pattern has happened or not. A collection of events should represent a patterns. As seen in the literature, these patterns are often a collection of two or three signals, which then has a followup market movement.

<!-- Input data -->

As explained in the previous section, we employ the interfaces provided by the algocore package to primarily access candle and indicator data. This is, of course, the least of what is expected, as any third party platform often also gives the ability to evaluate scripts. What we can do, on top of using market data, is to use other script events in other scripts.

Since the pipeline is able to instantly provide near real-time results, with a bit of overhead, we can use many scripts as dependencies to make each script focus on one task and make it easy to validate. This way we can create complexity out of simplicity with the thought of keeping everything as explainable as possible.

<!-- Events -->

The events that each script produces depend much on what to script is trying to achieve. In order to accommodate all types of events, the interface is used to create them as described in 5.3.2. Once the algorithm instance responds to the daemon, these are structured in an interface understood by all services which depend on the script results.

The pseudocode representation of the Event structure:

```
STRUCTURE Event
    FIELD CreatedOn: integer
    FIELD Time: integer
    FIELD Price: real number
    FIELD Label: string
    FIELD Icon: string
    FIELD Color: string
    FIELD Annotations: AnnotationCollection
```

An event happens at a given time and price. The script author can choose the time and price, since a script could predict an event, or mark one that happens in the past. The CreatedOn variable is, however, off limits for the author, since this variable is used to mask off the list of events when this is provided to another script. This way we can avoid any leaking of events that have future knowledge, but still make it able to show events that could happen in the future, like predicting tops.

In order to differentiate between events, we use a simple label field, which can then be used to label events like "uptrend" or "downtrend".

Fields "icon", "color" and "annotations" are used here to make visualization and validation easier. The color and icon fields are self-explanatory. The annotation field is used to store "sub-events", which contain extra information that we can use to validate an algorithm. In the interface that we specified, one can pass an event directly to these annotations, this way we can see if the right values of the dependency-algorithm are used when we evaluate the results of this algorithm.

These visualisation variables can be fully omitted in a production workflow, and would significantly reduce the computational and memory overhead; that is introduced by storing all this extra metadata.

Instead of thinking about the market represented by candles and their price, we can start thinking about the market as a series of temporal events. As opposed to simplifying the data by using methods like fourier transforms, we give meaning to price structures in the market.

<!-- Different type of scripts -->

In the methodology we will use a series of scripts, one set of scripts is used to extract features for pattern clustering, the other set is used to detect the patterns directly. These scripts have different goals, and therefore are differently implemented.

 - Clustering scripts: Take a common event based on an indicator to produce a market signal. Has a time and place.
 - Pattern scripts: Use the market price to determine technical analysis-like features, which can be used in combination to extract patterns.

Clustering scripts are therefore extensions of indicators. Whilst pattern scripts are, although simple, a more advanced technique of extracting events from the market. These pattern scripts also return data with some annotations to clearly visualize the pattern that is detected.

<!-- Stepper evaluation -->

As explained in the previous chapter (5.3.2), the scripts are always based on a stepper evaluation function. The scripts will always be evaluated over the whole history of a given asset. We call this function every step. Every step is defined by the requested interval for the algorithm. If the algorithm runs on an asset, it evaluates that asset's resolution, which is the lowest possible interval for that asset. This means that the algorithm has full control over what indicators it would like to use. The idea behind this is that the user of the script should pass all the parameters as part of the params, which can then be used to pass any intervals.

The types of the variables have been explained in the previous section. The MarketSupplier supplies data from other services, the ResultHandler is used to output events, Memory is used to store temporary data, and Parameters are used to access parameters passed to the script representing a given scenario.

Since we use the whole history, there is no point in having a warmup. The algorithm knows and can account for the first value in the asset's history, and thus no warmup logic is needed. Pattern scripts do have some warmup logic, since they look for structures in a series of candles, they first need to parse a few candles, and therefore will only work after some candles have been stepped through.

<!-- Parameters -->

In order to specify a script, how long things like these histories need to be, we use parameters. These parameters are always represented as floats, and can then be requested from within the script as an int, or a float.

This provides enough information for filling in unknowns as candle intervals, indicator parameters, or ranges in which to look for features. Since these parameters can represent floats, it could be used in an advanced case to evaluate trading strategies and pass parameters for profits and loss thresholds. In this thesis, however, we will exclusively use it to determine events in the context of patterns.

In the context of detecting Exponential Moving Average (EMA) crosses, one would specify the two ranges (such as 12 and 50) for which to detect crosses and the interval (in this case, the daily interval of 86400 seconds) on which to perform the detection. By passing these three parameters (12.0, 50.0, 86400), the algorithm can identify EMA crosses based on the specified ranges and an interval.

<!-- Memory -->

Depending on the type of function, it will need to have some memory. In all the scripts, we have to use memory, in order to know what state we are, so we can produce appropriate events. Initializing and loading memory could look as follows:

- Define a structure $Store$ that keeps track of the last EMA (Exponential Moving Average) cross state:
    - This structure has a state field $State$ used to represent the last state of the EMA lines, initially set to undefined.
- Declare a variable named $M$ of type $Store$.
- Attempt to read from memory, and store the result in $M$.
- If $M$ is null (the memory read did not return any value):
    - Initialize $M$ as a new instance of $Store$.
- Do any changes, related to the state of EMA, like changing $State$.
- After this of the candle step, we push the value of $M$ back to memory.

<!-- How the results are used -->

Given the structure of a script, one should be able to implement any algorithm. We then request the events for those algorithms from the clustering and manual pattern evaluators. In the case of clustering, we can use these events directly as features for the clustering; these clustering algorithms have no need for any other data. 

The evaluator which we use to check how well manual detection and clustering patterns are, can use the algocore package to request candles and the events which is used to see how well they perform. 

<!-- Missing data -->

In cases the market data is missing; just as with indicators, we will not step through that datapoint. This does not mean that we can ignore missing data in the script. We still have to check if data is missing when requesting candles from the data provider, as this provides any historical candle which could not exist. The same is true for indicators, which might only be available after its warmup period.

If we only use the current candle, then this does make it easier to script, since we can always assume that it is present. As for the evaluation, we also have to make sure that when retrieving the candle for which this event was created actually exists, as it is possible for events to be created in the future.

Failing to account for missing data can have significant consequences on the accuracy of output events, leading to wrong results. It is crucial to pay careful attention to missing candles, as there is no reliable method to detect their usage

<!-- Performance -->

The pipeline is designed to pass small blocks of data between services. The same cannot be said for algorithm's result sets. These results can be quite large when parsing events at a high resolution, or in cases where many events are detected. In order to avoid any bottlenecks, avoiding generating many events is the best cause of action. When every two candles produce an event, then a script is a bad fit for generating those events, and one should consider an approach by using indicators.

By using the metadata provided for each event, we can make sure that visualizing many events is done without impacting framerate in the visualization tool. This way we can omit events that are not visible to the user in a given block.

Another factor that involves performance is the use of the memory in a script. Since this memory is passed with a pointer, one should not be concerned when trying to store a lot of data in these structs. We will always evaluate algorithm linearly and not multithreading when stepping through a single scenario. This makes sure that we can use large structs in memory, which could be expanded to more advanced tasks like machine learning in each evaluation step and storing the model in memory.

<!-- Finish this chapter -->

With the established method for quickly generating and visualizing these events, we can check the accuracy of well-known events, such as EMA crosses and RSI thresholds. By comparing the charts and event placements with trusted platforms like TradingView, we can see if the events match what's typically expected from the literature.

It is important to mention that while the initial checks align with these third-party platforms, we have not carried out a full, detailed verification. The approach is based on the idea that if there were any issues, they'd likely show up as we use the system, giving us a real-world way to test its accuracy.

With this capability to pull out events from market data, the pipeline is now complete. We'll go into more detail about these events in the next methodology section, where we'll discuss the scripts we used.
