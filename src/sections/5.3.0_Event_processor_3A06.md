<!-- Introduction and purpose -->

The last daemon in the pipeline is the event processor. In order to do the actual pattern extraction, we needed to make it part of the pipeline. Since we already have the tools built in previous, we bundled these tools together in a package that can be used to write algorithm scripts. These algorithm scripts can then be compiled and spawned as workers by the algorithm daemon, which caches the results for each script, and takes away the hassle of having to build, run and cache programs in the actual experimentation phase.

We opted to consider each algorithm script as its own program, which implements the same package that contains all the necessary logic to be run by the daemon. This means that as someone writing and reviewing scripts, all the actual logistics of data can be left behind. We can access any candle block and any interval block by using an interface which hides all the logistics.

This way we can create easily explainable scripts that can detect patterns and run them on hundreds of scenarios in a few seconds times. Without the work of the previous services in the pipeline, this was simply not possible. Since we are also not limited by any constraints imposed by third-party platform, we can write any scripts without constraint.

When comparing to indicators, algorithms could be expressed as stateful variants of the expressions that were being evaluated in the indicator processor. Indicators are much more optimisation as they are short and stateless expressions which output their data in the same block format as candles. This makes it easy to display indicator data alongside candle data.

When working with algorithms, we opted to use events instead of mapping candle points. This means that we can store much more information per event compared to a data point used in an indicator. As this data is also sparse, it can still be transferred quite quickly and displayed in the visualisation tool. These events also include a lot of data that we can use to make better visualisations that are not possible with conventional indicators.

In the next section, we will explain how we ended up with the framework used to calculate the algorithms. We will first have a look at the server structure used to spawn the worker processes. Next, we will explain how the algorithm helper package is structured, and the logic behind it. Finally, we will show some examples of the actual algorithm scripts that are being used.
