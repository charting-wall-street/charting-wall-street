Given that the total number of patterns in the dataset is unknown, and the objective is to discover new patterns rather than pre-existing ones, we initially attempt to run the algorithm on the entire dataset. However, this approach proves impractical due to the complexity of handling a large number of patterns, which amounted to around 500k+ with our initial parameters of a window size of 14 days and an overlap of 2 days. While we are uncertain about the quantity of patterns of a specific type, it is necessary to discover a significant number of instances of a given pattern to obtain meaningful results. This presents a dilemma where we must make a trade-off between reducing the number of assets or reducing the number of samples, which would also decrease the chances of finding patterns.

**Merging identical patterns:**

To address the challenge of reducing the number of extracted patterns from the complete dataset, the most effective approach is to merge patterns with a distance of zero. This merging process is performed recursively and can be executed using multiple threads, resulting in a substantial reduction in the number of patterns. During this process, the patterns are converted into nodes, and an associated count is maintained to ensure the preservation of this metric.

**Sampling patterns:**

To improve memory consumption, another approach involves sampling patterns from the source dataset. By selecting a predetermined ratio of desired samples relative to the total number of samples, we can reduce the working set of patterns. However, it is important to note the reduction in the number of unique patterns. Nevertheless, the resulting groups after merging, as explained in the previous paragraph, remain comparable to the number of nodes.

**Unique pattern identifiers:**

An attempted optimization that proves unsuccessful in practice is assigning a unique identifier to each pattern using a hashing algorithm. The aim is to simplify the merging of equivalent patterns. However, due to the temporal warping of patterns, accounting for this aspect in the hashing process is deemed infeasible, leading to the abandonment of this optimization approach.

**Summary:**

A combination of merging identical patterns and sampling proves effective in reducing the number of patterns to a manageable working set, enhancing computational efficiency. However, the effects of sample size should be further explored to determine an appropriate ratio.

Despite the optimizations applied to clustering time, the process remains insufficiently fast for efficient hyperparameter optimization. The identified list of parameters includes:

- Number of indicators.
- Window size of the pattern.
- Overlap between windows.
- Resolution of the candles (e.g., 1h vs 30m).
- Parameters for specific indicators.
- Correlation between indicators.
- Interval of indicators.
- Window size for distance calculations.

- Given the extensive nature of these parameters, a highly efficient clustering approach would be preferable to facilitate the optimization of hyperparameters. This efficient approach would enable the identification of useful patterns and the reduction of noise in the patterns discovered through educated guesswork. By efficiently optimizing these hyperparameters, we can enhance the effectiveness of pattern identification and analysis in the context of the research.