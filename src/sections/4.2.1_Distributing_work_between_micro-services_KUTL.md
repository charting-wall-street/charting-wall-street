A microservices architecture is employed to distribute the workload across specialized services [@Salah_Jamal]. This approach enables quick modifications and restarts of individual services without losing any in-memory data stored by other services. Moreover, it facilitates streamlined testing, as services can be promptly patched and restarted in the event of a crash. This flexibility is crucial for rapid evaluation of new features and components intended for integration into the pipeline.

The pipeline consists of three main services and one helper service (Figure \ref{fig:platform_topology}):

![An overview of the services and their dataflow directions. Core services are exposed, and are used for visualizations and evaluating the patterns.](../figures/platform_topology.png){#fig:platform_topology width=50%}

1. Bridge Service: Acts as an interface between external APIs and internal services.
2. Candle Processor: Stores and processes candle data, providing it in multiple time intervals.
3. Indicator Processor: Processes candles to generate commonly used market indicators.
4. Event Processor: Manages and executes algorithm scripts, eliminating the need for manual startup.

During the development of the candle and indicator services, it became evident that storing intermediate results would play a pivotal role, especially given the challenges posed by microservice data management, including ensuring data consistency and efficiently querying database states [@Laigner_Zhou_Salles_Liu_Kalinowski_2021]. Initially, caching was implemented to store blocks of candles loaded from disk. However, due to the static nature of historical data, caching could be safely incorporated at each stage of derivative calculation, optimizing performance.

To optimize caching, we employ a medium-sized cache that follows the least-recently used principle. This cache efficiently stores a variety of data, such as raw blocks of candles, blocks of candles calculated for different time intervals, and blocks of indicators, along with their respective parameters and exchange metadata. By including frequently used metadata, we can more easily determine the start and end dates when needed.

To further enhance efficiency, service requests for data are designed to avoid redundant requests. When a service makes a request, it prevents other threads from making the same request concurrently. Once the request is fulfilled, the data is retained in memory to minimize the overhead of redundant data requests. Reducing the number of data exchanges significantly improves performance, as the primary overhead in calculating algorithms and indicators lies in data exchange.

For instance, caches are implemented at both endpoints, minimizing traffic and reducing overhead associated with data requests beyond the initial request. This approach is inspired by the practices employed in web caching, where strategic document caching is essential to decrease download times and reduce network traffic [@Balamash_Krunz_Second_2004]. In much the same way as web caching policies are designed to adapt to changing workloads, the caching strategy in this research has been tailored to respond to dynamic demands and characteristics of the stock data workload.