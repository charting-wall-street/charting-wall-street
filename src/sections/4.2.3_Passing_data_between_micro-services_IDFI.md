<!-- Data Exchange Protocol -->

This thesis describes several microservices that employ a consistent data exchange structure. The services utilize HTTP requests for interaction, both from client applications to services and between services themselves. The decision to use HTTP, rather than alternative technologies such as web frameworks, was motivated by the desire to avoid complexities associated with data exchange. Specifically, the choice of HTTP was driven by the availability of visualization and debugging tools that support this protocol.

The visualization tool, which is an integral part of the system, also relies on HTTP, enabling the straightforward implementation of a fetcher to retrieve data from the services. This facilitates result verification for each service without requiring substantial additional effort to implement receiver and sender logic. However, it should be noted that the tools developed in this thesis are not suitable for production environments where service endpoints may be publicly accessible. This limitation arises from the fact that the services were not evaluated for security and lack measures to prevent abuse.

To handle HTTP requests efficiently, two publicly available libraries were utilized. The first library, gorilla/mux [@mux], serves as a request router, allowing developers to define and handle complex routing patterns and URL parameters in web applications. The second library, urfave/negroni [@negroni], acts as middleware, providing a modular and user-friendly framework for managing HTTP middleware functions. The gorilla/mux library is employed to configure endpoints and handle tasks such as Cross-Origin Resource Sharing (CORS) required for communication between the visualization tool and the services. Negroni, on the other hand, is used primarily for error recovery to prevent server crashes caused by panics in request routines.

<!-- Data Serialization -->
To simplify data exchange, structs were created and shared between services through centralized packages containing their definitions. This approach enables straightforward encoding and decoding of data streams based on the known struct being passed. Depending on the "Accept" header in the HTTP request, the response is sent in either JSON or binary format. The binary format was introduced to eliminate the overhead associated with JSON decoding [@Miretskiy_2023]. By default, services communicate with each other using binary encoding, while the visualization tool exclusively uses JSON since it serves as an auxiliary tool where performance can be sacrificed. The adoption of binary encoding reduces the overhead introduced by JSON encoding.

In most cases, Go's gob package [@go_gob] is utilized for binary encoding. This package offers the capability to encode and decode values in a compact binary format, which facilitates data serialization for transmission over networks or for file storage. When encoding candles using the gob package, the overhead is reduced to mere single milliseconds, as opposed to multiple milliseconds. However, an inherent inefficiency emerges when one needs to extract specific data points: the entire block must be retrieved each time. This process, when repeated multiple times to extract a few candles from different blocks, underscores the fact that the gob package, while efficient, still introduces discernible overhead. To address this inefficiency, a custom binary encoder and decoder were developed. It is important to highlight that binary encoding is solely used for encoding candle and indicator blocks, as they represent the majority of the data exchanged between services.

The binary encoding structure for candle blocks is illustrated in the following table:

| Offset (bytes)       | Encoded Data                                   |
|----------------------|------------------------------------------------|
| 0                    | Number of Candles (uint64, big endian)         |
| 8                    | Candle 1 - Open (float64, big endian)          |
| 16                   | Candle 1 - High (float64, big endian)          |
| 24                   | Candle 1 - Low (float64, big endian)           |
| 32                   | Candle 1 - Close (float64, big endian)         |
| 40                   | Candle 1 - Volume (float64, big endian)        |
| 48                   | Candle 1 - TakerVolume (float64, big endian)   |
| 56                   | Candle 1 - NumberOfTrades (uint64, big endian) |
| 64                   | Candle 1 - Time (uint64, big endian)           |
| 72                   | Candle 1 - Missing (uint8)                     |
| ...                  | ...                                            |
| 8 + cSize*(i-1)      | Candle i - Open (float64, big endian)          |
| 16 + cSize*(i-1)     | Candle i - High (float64, big endian)          |
| 24 + cSize*(i-1)     | Candle i - Low (float64, big endian)           |
| 32 + cSize*(i-1)     | Candle i - Close (float64, big endian)         |
| 40 + cSize*(i-1)     | Candle i - Volume (float64, big endian)        |
| 48 + cSize*(i-1)     | Candle i - TakerVolume (float64, big endian)   |
| 56 + cSize*(i-1)     | Candle i - NumberOfTrades (uint64, big endian) |
| 64 + cSize*(i-1)     | Candle i - Time (uint64, big endian)           |
| 72 + cSize*(i-1)     | Candle i - Missing (uint8)                     |
| ...                  | ...                                            |
| 8 + cSize*(n-1)      | Candle n - Open (float64, big endian)          |
| 16 + cSize*(n-1)     | Candle n - High (float64, big endian)          |
| 24 + cSize*(n-1)     | Candle n - Low (float64, big endian)           |
| 32 + cSize*(n-1)     | Candle n - Close (float64, big endian)         |
| 40 + cSize*(n-1)     | Candle n - Volume (float64, big endian)        |
| 48 + cSize*(n-1)     | Candle n - TakerVolume (float64, big endian)   |
| 56 + cSize*(n-1)     | Candle n - NumberOfTrades (uint64, big endian) |
| 64 + cSize*(n-1)     | Candle n - Time (uint64, big endian)           |
| 72 + cSize*(n-1)     | Candle n - Missing (uint8)                     |
| Offset after Candles | Meta Data (serialized using gob)               |

In testing, when requesting data from the candle service where the requested payload is cached, the average response times were as follows: 25ms using JSON, 12ms using gob, and 5ms using binary encoding (evaluated using Go's built-in benchmarking toolkit).

Further optimization efforts to reduce the remaining 5ms would require significant investment. Within this remaining time, approximately 2ms can be attributed to server response time, 2ms to payload reading, and 1ms to the decoding algorithm. The binary encoding and decoding technique employed in this thesis significantly reduced overhead time, as hundreds of requests are executed per second when requesting results at the end of the pipeline. Although additional optimization may be possible, such efforts are beyond the scope of this thesis.

A similar approach to encoding and decoding data is used for indicators. However, since indicator blocks are exchanged less frequently, a detailed discussion is omitted. The payload structure for indicators is as follows:


| Offset (bytes) | Encoded Data                                |
|----------------|---------------------------------------------|
| 0-7            | Number of series (uint64)                   |
| 8 - n          | Series Data (variable)                      |
|                | --- Series Key                              |
|                | --- Series Kind                             |
|                | --- Series Axis                             |
|                | --- Series Values (Repeated IndicatorValue) |
|                | ------ Value (float64)                      |
|                | ------ Missing (bool)                       |
| n+1 - end      | Meta Data (gob encoded IndicatorMeta)       |

Although binary encoding effectively reduces overhead, it poses challenges when expanding data structures with additional information. As evident from the encoding table, gob encoding is still used for the metadata section, as the finality of the metadata and the potential for changes remained uncertain. Therefore, binary encoding was selectively implemented where it provided the most significant benefits. In all other data exchanges, JSON or gob encoding is still employed, as developing a custom encoder for each scenario would not be justifiable given the time investment required for implementation and testing.

<!-- Data Synchronization -->

The use of HTTP requests and the optimizations applied to encoding and decoding processes expose certain system limits, particularly related to file descriptors. When payloads are requested but not properly closed, the corresponding connections remain open for a certain period. As the system makes hundreds to thousands of requests per second, this issue becomes problematic as file descriptors accumulate, waiting for subsequent requests. It was observed that these open TCP connections are not reused when multiple requests are performed in parallel, leading to undesired consequences. The official documentation of the http library states:

"If the returned error is nil, the Response will contain a non-nil Body which the user is expected to close. If the Body is not both read to EOF and closed, the Client's underlying RoundTripper (typically Transport) may not be able to re-use a persistent TCP connection to the server for a subsequent 'keep-alive' request" [@go_http].

To address potential issues with persistent TCP connections, it is essential to ensure that the request body is drained before closure. Neglecting this order can lead to numerous redundant persistent TCP connections.

```go
// Drain any remaining data in the response body
if _, err = io.Copy(io.Discard, resp.Body); err != nil {
    log.Fatal(err)
}
// Close the response body, which should also close the TCP connection
if err = resp.Body.Close(); err != nil {
    log.Fatal(err)
}
```

By addressing file descriptor issues and optimizing JSON serialization overhead, the system overhead has been notably reduced. This optimization paves the way for scaling the pipeline without facing data exchange constraints. Caching, by eliminating repetitive data fetches, further boosts the efficiency of the pipeline. The upcoming chapter on processing delves deeper into caching mechanisms.

To wrap up this section, the detailed exposition of the data exchange methodologies underscores a commitment to both efficiency and adaptability. By carefully selecting encoding techniques and streamlining system interactions, this framework not only ensures the effective support of the subsequent stages of this research but also provides valuable groundwork for real-world scenarios involving the analysis of stock market data.