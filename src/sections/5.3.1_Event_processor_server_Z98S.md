Initially, we went with a queue approach where a client would be able to request a job, which would be the calculation of an algorithm. The client could then check up on this job, and once the job would be finished, the client would be able to request the result for said job. This, however, involved a lot of coordinating between the client and the service, and also between the service and the worker. The initial reason for taking this approach was because algorithms took a long time to calculate. But with the many optimizations, the actual time it took the worker to evaluate a script was less than the time it took for all the overhead communications to take place. We, therefore, went back on this approach in favor of simple synchronous requests.

When a client requests the results for an algorithm, we calculate all the events from the start of the requested asset's history till the end. The service waits for the worker to finish and forwards its results back to the client. If the worker is not yet running, it will be compiled and started. Once the worker has started, it will stay running until the termination of the service.

In most cases, keeping the worker running is fine. But when making changes or debugging one of the algorithm's scripts, we would like to request the results for the new version of the script. We, therefore, add a special parameter that can be passed to indicate to the service that it should stop the existing worker, build it again, and start it. The difficulty with this is that both the worker and service will accept and run multiple requests in parallel, meaning that we cannot just stop the worker at any time without risking data loss.

To coordinate restarts, we send a termination signal to the worker and hold any incoming requests for that worker. The worker will now enter its termination state. Once it has finished all running requests, it will gracefully terminate, and with a successful exit signal, the service will know that it can safely rebuild and restart it. Since Go makes it possible to easily implement thread-safe variables, we can easily coordinate this termination and restart using channels to indicate termination, and locks to make sure no additional requests are passed when the server is restarting.

We are now able to build and run any script and restart it safely at any time. This makes it possible for another service to simply request the events of an algorithm by sending a simple HTTP request. It not only simplifies the final experimentation but also makes it possible to use algorithm results in other algorithm scripts.

Ideally, we do not want to have any caching steps at the end of the pipeline, and therefore implement all possible caching on the server. In order to make memory management concerning the cache of this result as convenient as possible, we cache all the results in the event processor. Since we know the number of events and how much an event will take up in memory, we can make a good guess for each result that we store in the cache. This way, we can effectively store recent results for a long time to make sure the experiments do not request a result that needs to be computed twice. The cache is set up in the exact same way as for the candle service, so we will not discuss it any further here.

